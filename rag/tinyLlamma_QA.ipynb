{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Cta-s3j7vkA"
      },
      "outputs": [],
      "source": [
        "!pip install -q  transformers sentence-transformers torch langchain-community bitsandbytes langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Web Crawling and Scraping:** Ability to crawl and scrape a website for LLM RAG, with deduplication of data between pages.\n",
        "2. **PDF Support:** Extract and process data from PDFs.\n",
        "3. **QA Support:** Implement a question-answering functionality.\n",
        "4. **Text Blob Support:** Handle and process large text blobs.\n",
        "\n",
        "The system should utilize FAISS for efficient indexing and OpenAI for embedding and fetching data. The final deliverable is a fully functional Jupyter Notebook in Google Colab that demonstrates these capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9En43Ag7vkC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline,BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "def load_llm(model_name: str=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "             device: str='cuda') -> HuggingFacePipeline:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.3,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=pipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me4hgel97vkD"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-community  langchain-openai faiss-cpu pypdf nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll3GMCqMDOXP",
        "outputId": "dc65c4b6-6403-42cf-b5dd-963fdb220aac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8bTzCVY7vkD",
        "outputId": "c6f67686-7273-4e2f-c426-cf62759b41c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import AsyncHtmlLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "from typing import List, Union\n",
        "from langchain.schema import Document\n",
        "\n",
        "def load_documents(sources: List[Union[str, dict]]) -> List[Document]:\n",
        "    documents = []\n",
        "    for source in sources:\n",
        "        if isinstance(source, str) and source.startswith(\"http\"):\n",
        "            loader = AsyncHtmlLoader([source])\n",
        "            documents.extend(loader.load())\n",
        "        elif isinstance(source, str) and source.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(source)\n",
        "            documents.extend(loader.load())\n",
        "        elif isinstance(source, dict) and \"url\" in source:\n",
        "            loader = AsyncHtmlLoader([source[\"url\"]])\n",
        "            docs = loader.load()\n",
        "            for doc in docs:\n",
        "                doc.metadata.update(source.get(\"metadata\", {}))\n",
        "            documents.extend(docs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported source type: {source}\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def process_with_textblob(document: Document) -> Document:\n",
        "    blob = TextBlob(document.page_content)\n",
        "\n",
        "    # Perform TextBlob analysis\n",
        "    sentiment = blob.sentiment\n",
        "    noun_phrases = blob.noun_phrases\n",
        "\n",
        "    # Add TextBlob analysis to document metadata\n",
        "    document.metadata[\"sentiment_polarity\"] = sentiment.polarity\n",
        "    document.metadata[\"sentiment_subjectivity\"] = sentiment.subjectivity\n",
        "    document.metadata[\"noun_phrases\"] = noun_phrases[:5]  # Limit to top 5 noun phrases\n",
        "\n",
        "    return document\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    documents: List[Document],\n",
        "    chunk_size: int = 300,\n",
        "    chunk_overlap: int = 30\n",
        ") -> List[Document]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(documents)\n",
        "    processed_docs = [process_with_textblob(doc) for doc in split_docs]\n",
        "    return processed_docs\n",
        "\n",
        "\n",
        "def create_vectorstore(texts: List[Document],embeddings_model_name: str=\"text-embedding-3-small\") -> FAISS:\n",
        "    embeddings = OpenAIEmbeddings(model=embeddings_model_name)\n",
        "    return FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0hfXkbh7vkE"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "\n",
        "def create_qa_chain(vectorstore: FAISS, llm: HuggingFacePipeline) -> RetrievalQA:\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Bl3xpbD8wN"
      },
      "source": [
        "# web Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW7auqUu8cfA",
        "outputId": "295c365e-7863-4524-ac4f-f7489b3b8d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  5.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1 documents\n",
            "Splitting documents...\n",
            "Created 1016 document chunks\n",
            "Creating vector store...\n",
            "Loading language model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating QA chain...\n",
            "Generating answer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer: Use the following pieces of context to answer the question at the end. \n",
            "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "    pip install llama</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">-</span><span class=\"token plain\">cpp</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">-</span><span class=\"token plain\">python</span><br></span></code></pre><div class=\"buttonGroup__atx\"><button\n",
            "\n",
            "to Compiling and installing\" title=\"Direct link to Compiling and installing\">​</a></h4><p>Now you can <code>cd</code> into the <code>llama-cpp-python</code> directory and install the package</p><div class=\"codeBlockContainer_Ckt0 theme-code-block\"\n",
            "\n",
            "class=\"token plain\"> pip install llama</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">-</span><span class=\"token plain\">cpp</span><span class=\"token operator\" style=\"color:rgb(0, 0, 0)\">-</span><span class=\"token plain\">python</span><br></span></code></pre><div\n",
            "\n",
            "link to Installation\" title=\"Direct link to Installation\">​</a></h2><p>There are different options on how to install the llama-cpp package: </p><ul><li>CPU usage</li><li>CPU + GPU (using one of many BLAS backends)</li><li>Metal GPU (MacOS with Apple Silicon Chip) </li></ul><h3 class=\"anchor\n",
            "\n",
            "    Question: Are any option install llama-cpp package?\n",
            "    Answer: Yes, there are several ways to install the llama-cpp package: CPU usage, CPU + GPU, Metal GPU.</h3><ol><li>CPU usage - You can use the command line tool \"pip install llama\" to install the package directly from your terminal or command prompt. This will download the package and its dependencies automatically. The installation process may take a few minutes depending on your internet connection speed. Once installed, you can run the package using Python by importing it in your code. For example:</li><li>CPU + GPU - To install the llama-cpp package for CPU + GPU, you need to have both CPU and GPU available on your system. Here’s how to do this:</li><li>1. Download the latest version of LLVM from https://releases.llvm.org/download.html. Make sure to choose the appropriate version for your operating system. 2. Extract the downloaded file to a folder of your choice. 3. Open a terminal window and navigate to the extracted folder. 4. Run the following command to build the C++ backend:</li><li>```bash\n",
            "$ cd llama-cpp-backend\n",
            "$ mkdir build && cd build\n",
            "$ cmake..\n",
            "$ make\n",
            "```\n",
            "This will compile the C++ backend for LLVM. 5. Run the following command to build the Python frontend:</li><li>```bash\n",
            "$ cd llama-py\n",
            "$ python setup.py develop\n",
            "```\n",
            "This will install the Python frontend as a development dependency. 6. Now you can import the llama module in your Python code like this:</li><li>```python\n",
            "import llama\n",
            "```\n",
            "This will load the llama module and its associated classes and functions. 7. Finally, you can test the llama-cpp package by running the following Python code:</li><li>```python\n",
            "from llama import *\n",
            "print(\"Hello World!\")\n",
            "```\n",
            "This will execute the Hello World program provided in the llama-cpp repository. I hope these instructions help you get started with building and installing the llama-cpp package. Let me know if you have any further questions. Best regards,\n",
            "[Your Name]\n"
          ]
        }
      ],
      "source": [
        "# web Query\n",
        "urls = [\n",
        "    \"https://python.langchain.com/docs/integrations/llms/llamacpp\"\n",
        "]\n",
        "print(\"Loading documents...\")\n",
        "documents = load_documents(urls)\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "\n",
        "print(\"Splitting documents...\")\n",
        "split_docs = split_documents(documents)\n",
        "print(f\"Created {len(split_docs)} document chunks\")\n",
        "\n",
        "print(\"Creating vector store...\")\n",
        "vectorstore = create_vectorstore(split_docs)\n",
        "\n",
        "print(\"Loading language model...\")\n",
        "llm = load_llm()\n",
        "\n",
        "print(\"Creating QA chain...\")\n",
        "qa_chain = create_qa_chain(vectorstore, llm)\n",
        "\n",
        "print(\"Generating answer...\")\n",
        "result = qa_chain.invoke({\"query\": \"Are any option install llama-cpp package?\"})\n",
        "print(f\"\\nAnswer: {result['result']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
